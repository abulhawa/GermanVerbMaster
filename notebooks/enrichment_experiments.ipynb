{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee19f365",
   "metadata": {},
   "source": [
    "# Enrichment API Experiments\n",
    "\n",
    "This notebook lets you explore the external datasources used by the German Verb Master enrichment pipeline without running the application server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a0db36",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Install the packages below in your Python environment before running the notebook:\n",
    "\n",
    "- Python 3.10 or newer\n",
    "- [`requests`](https://pypi.org/project/requests/) for HTTP calls\n",
    "- [`python-dotenv`](https://pypi.org/project/python-dotenv/) *(optional, load API keys from a `.env` file)*\n",
    "- [`pydantic`](https://pypi.org/project/pydantic/) *(optional, validate parsed payloads)*\n",
    "\n",
    "Environment variables (only needed for the OpenAI helper cell):\n",
    "\n",
    "- `OPENAI_API_KEY` — secret token for the Chat Completions API\n",
    "\n",
    "All other datasources are public and can be queried without credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1257cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from dataclasses import dataclass, field\n",
    "from html import unescape\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import requests\n",
    "\n",
    "REQUEST_HEADERS = {\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"User-Agent\": \"GermanVerbMaster/1.0 (data enrichment experiments)\",\n",
    "}\n",
    "\n",
    "WIKTIONARY_API = \"https://de.wiktionary.org/w/api.php\"\n",
    "OPEN_THESAURUS_API = \"https://www.openthesaurus.de/synonyme/search\"\n",
    "MY_MEMORY_API = \"https://api.mymemory.translated.net/get\"\n",
    "TATOEBA_API = \"https://tatoeba.org/en/api_v0/search\"\n",
    "WIKTEXTRACT_SEARCH_API = \"https://kaikki.org/dewiktionary/search/start\"\n",
    "WIKTEXTRACT_RAW_MARKER = \"[Show JSON for raw wiktextract data ▼]\"\n",
    "OPENAI_CHAT_COMPLETIONS = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "@dataclass\n",
    "class WiktionaryVerbForms:\n",
    "    praeteritum: Optional[str] = None\n",
    "    partizip_ii: Optional[str] = None\n",
    "    perfekt: Optional[str] = None\n",
    "    aux: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class WiktionaryLookup:\n",
    "    summary: Optional[str]\n",
    "    english_hints: List[str]\n",
    "    forms: Optional[WiktionaryVerbForms]\n",
    "\n",
    "@dataclass\n",
    "class SynonymLookup:\n",
    "    synonyms: List[str]\n",
    "\n",
    "@dataclass\n",
    "class TranslationLookup:\n",
    "    translation: Optional[str]\n",
    "    source: str\n",
    "    confidence: Optional[float]\n",
    "\n",
    "@dataclass\n",
    "class ExampleLookup:\n",
    "    example_de: Optional[str]\n",
    "    example_en: Optional[str]\n",
    "    source: str\n",
    "\n",
    "@dataclass\n",
    "class WiktextractLookup:\n",
    "    translations: List[str]\n",
    "    synonyms: List[str]\n",
    "    example: Optional[ExampleLookup]\n",
    "\n",
    "@dataclass\n",
    "class RequestRecord:\n",
    "    method: str\n",
    "    url: str\n",
    "    params: Dict[str, Any]\n",
    "    headers: Dict[str, Any]\n",
    "    body: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class ProviderResponse:\n",
    "    request: RequestRecord\n",
    "    raw: Any\n",
    "    parsed: Any\n",
    "    history: List[RequestRecord] = field(default_factory=list)\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "\n",
    "def prepare_request(method: str, url: str, *, params: Optional[Dict[str, Any]] = None,\n",
    "                    headers: Optional[Dict[str, str]] = None, json_body: Any = None) -> Tuple[RequestRecord, requests.PreparedRequest]:\n",
    "    request = requests.Request(method=method, url=url, params=params,\n",
    "                               headers=headers or REQUEST_HEADERS,\n",
    "                               json=json_body)\n",
    "    prepared = session.prepare_request(request)\n",
    "    record = RequestRecord(\n",
    "        method=method,\n",
    "        url=prepared.url,\n",
    "        params=params or {},\n",
    "        headers=dict(prepared.headers),\n",
    "        body=prepared.body.decode(\"utf-8\") if isinstance(prepared.body, bytes) else prepared.body,\n",
    "    )\n",
    "    return record, prepared\n",
    "\n",
    "\n",
    "def send_request(prepared: requests.PreparedRequest) -> requests.Response:\n",
    "    response = session.send(prepared)\n",
    "    response.raise_for_status()\n",
    "    return response\n",
    "\n",
    "\n",
    "def to_array(value: Any) -> List[Any]:\n",
    "    return value if isinstance(value, list) else []\n",
    "\n",
    "\n",
    "def split_template_arguments(template: str) -> List[str]:\n",
    "    content = template[2:-2]\n",
    "    parts: List[str] = []\n",
    "    buffer: List[str] = []\n",
    "    depth = 0\n",
    "    i = 0\n",
    "    while i < len(content):\n",
    "        char = content[i]\n",
    "        nxt = content[i + 1] if i + 1 < len(content) else \"\"\n",
    "        if char == \"{\" and nxt == \"{\":\n",
    "            depth += 1\n",
    "            buffer.append(\"{{\")\n",
    "            i += 2\n",
    "            continue\n",
    "        if char == \"}\" and nxt == \"}\":\n",
    "            depth = max(depth - 1, 0)\n",
    "            buffer.append(\"}}\")\n",
    "            i += 2\n",
    "            continue\n",
    "        if char == \"|\" and depth == 0:\n",
    "            parts.append(\"\".join(buffer).strip())\n",
    "            buffer = []\n",
    "            i += 1\n",
    "            continue\n",
    "        buffer.append(char)\n",
    "        i += 1\n",
    "    if buffer:\n",
    "        parts.append(\"\".join(buffer).strip())\n",
    "    return [part for part in parts if part]\n",
    "\n",
    "\n",
    "def extract_template(content: str, template_name: str) -> Optional[str]:\n",
    "    needle = f\"{{{{{template_name}\"\n",
    "    start = content.find(needle)\n",
    "    if start == -1:\n",
    "        return None\n",
    "    depth = 0\n",
    "    i = start\n",
    "    while i < len(content) - 1:\n",
    "        if content[i] == \"{\" and content[i + 1] == \"{\":\n",
    "            depth += 1\n",
    "            i += 2\n",
    "            continue\n",
    "        if content[i] == \"}\" and content[i + 1] == \"}\":\n",
    "            depth -= 1\n",
    "            i += 2\n",
    "            if depth == 0:\n",
    "                return content[start:i + 1]\n",
    "            continue\n",
    "        i += 1\n",
    "    return None\n",
    "\n",
    "\n",
    "def strip_wiki_markup(value: str) -> str:\n",
    "    result = re.sub(r\"'''\", \"\", value)\n",
    "    result = re.sub(r\"''\", \"\", result)\n",
    "    result = re.sub(r\"<[^>]+>\", \" \", result)\n",
    "    result = result.replace(\"&nbsp;\", \" \")\n",
    "    result = re.sub(r\"\\{\\{(?:[lL]|link-de)\\|[^|}]+\\|([^}|]+)(?:\\|[^}]*)*}}\", r\"\\1\", result)\n",
    "    result = re.sub(r\"\\{\\{lang\\|[^|}]+\\|([^}]+)}}\", r\"\\1\", result)\n",
    "    result = re.sub(r\"\\{\\{[^}]+}}\", \" \", result)\n",
    "    result = re.sub(r\"\\[\\[([^|\\]]*\\|)?([^\\]]+)]]\", r\"\\2\", result)\n",
    "    result = re.sub(r\"\\s+\", \" \", result)\n",
    "    return result.strip()\n",
    "\n",
    "\n",
    "def normalise_key(value: str) -> str:\n",
    "    import unicodedata\n",
    "\n",
    "    normalised = unicodedata.normalize(\"NFD\", value)\n",
    "    normalised = \"\".join(ch for ch in normalised if unicodedata.category(ch) != \"Mn\")\n",
    "    normalised = normalised.lower()\n",
    "    normalised = re.sub(r\"[^a-z0-9]+\", \"\", normalised)\n",
    "    return normalised\n",
    "\n",
    "\n",
    "def normalise_auxiliary(value: Optional[str]) -> Optional[str]:\n",
    "    if not value:\n",
    "        return None\n",
    "    normalised = value.strip().lower()\n",
    "    if \"sein\" in normalised:\n",
    "        return \"sein\"\n",
    "    if \"haben\" in normalised:\n",
    "        return \"haben\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def select_primary(value: Optional[str]) -> Optional[str]:\n",
    "    if not value:\n",
    "        return None\n",
    "    parts = re.split(r\"[;,/]| oder | bzw\\.| bzw \", value)\n",
    "    cleaned = [strip_wiki_markup(part).strip() for part in parts]\n",
    "    cleaned = [part for part in cleaned if part]\n",
    "    return (cleaned[0] if cleaned else strip_wiki_markup(value).strip()) or None\n",
    "\n",
    "\n",
    "def match_from_markup(content: str, pattern: re.Pattern[str]) -> Optional[str]:\n",
    "    match = pattern.search(content)\n",
    "    if not match:\n",
    "        return None\n",
    "    return strip_wiki_markup(match.group(1) if match.lastindex else \"\")\n",
    "\n",
    "\n",
    "def extract_verb_forms(content: str) -> Optional[WiktionaryVerbForms]:\n",
    "    template_names = [\"Deutsch Verb Übersicht\", \"Deutsch Verb Flexion\"]\n",
    "    forms = WiktionaryVerbForms()\n",
    "    collected_keys: Dict[str, str] = {}\n",
    "    positional: List[str] = []\n",
    "\n",
    "    for name in template_names:\n",
    "        template = extract_template(content, name)\n",
    "        if not template:\n",
    "            continue\n",
    "        parts = split_template_arguments(template)\n",
    "        for part in parts[1:]:\n",
    "            if \"=\" not in part:\n",
    "                positional.append(strip_wiki_markup(part))\n",
    "                continue\n",
    "            key, value = part.split(\"=\", 1)\n",
    "            key = normalise_key(key)\n",
    "            value = strip_wiki_markup(value)\n",
    "            if key and value:\n",
    "                collected_keys[key] = value.strip()\n",
    "\n",
    "    def first(*selectors: Any) -> Optional[str]:\n",
    "        for selector in selectors:\n",
    "            if isinstance(selector, str):\n",
    "                value = collected_keys.get(selector)\n",
    "                if value:\n",
    "                    return value\n",
    "            elif isinstance(selector, re.Pattern):\n",
    "                for key, value in collected_keys.items():\n",
    "                    if selector.search(key) and value:\n",
    "                        return value\n",
    "        return None\n",
    "\n",
    "    forms.praeteritum = select_primary(\n",
    "        first(\"prateritumich\", \"praeteritumich\", \"prateritum\", \"praeteritum\", re.compile(r\"^(praeteritum|prateritum)\"))\n",
    "        or (positional[2] if len(positional) > 2 else None)\n",
    "    )\n",
    "    forms.partizip_ii = select_primary(\n",
    "        first(\"partizipii\", \"partizip2\", \"partizipii1\", re.compile(r\"^(partizip|partizipii|partizip2)\"))\n",
    "        or (positional[3] if len(positional) > 3 else None)\n",
    "    )\n",
    "    aux_value = first(\"hilfsverb\", \"auxiliar\", \"auxiliary\", \"perfektauxiliar\",\n",
    "                      re.compile(r\"^hilfsverb\"), re.compile(r\"^aux\"))\n",
    "    forms.aux = normalise_auxiliary(aux_value)\n",
    "\n",
    "    perfekt_candidate = first(\"perfektich\", \"perfektwir\", \"perfekt\", \"perfekt1\", re.compile(r\"^perfekt\"))\n",
    "    if not perfekt_candidate and len(positional) > 4:\n",
    "        perfekt_candidate = positional[4]\n",
    "    forms.perfekt = select_primary(perfekt_candidate)\n",
    "\n",
    "    if not forms.praeteritum:\n",
    "        forms.praeteritum = select_primary(match_from_markup(content, re.compile(r\"'''Pr[aä]teritum:?'''([^\n",
    "]+)\", re.I)))\n",
    "    if not forms.partizip_ii:\n",
    "        forms.partizip_ii = select_primary(match_from_markup(content, re.compile(r\"'''Partizip(?:\\s+II)?[:]?'''([^\n",
    "]+)\", re.I)))\n",
    "    if not forms.aux:\n",
    "        forms.aux = normalise_auxiliary(match_from_markup(content, re.compile(r\"'''Hilfsverb:?'''([^\n",
    "]+)\", re.I)))\n",
    "    if not forms.perfekt:\n",
    "        perfekt_text = match_from_markup(content, re.compile(r\"'''Perfekt:?'''([^\n",
    "]+)\", re.I))\n",
    "        forms.perfekt = select_primary(perfekt_text)\n",
    "\n",
    "    if not forms.perfekt and forms.partizip_ii and forms.aux:\n",
    "        auxiliary = \"ist\" if forms.aux == \"sein\" else \"hat\"\n",
    "        forms.perfekt = f\"{auxiliary} {forms.partizip_ii}\"\n",
    "\n",
    "    if forms.partizip_ii:\n",
    "        forms.partizip_ii = select_primary(forms.partizip_ii)\n",
    "    if forms.praeteritum:\n",
    "        forms.praeteritum = select_primary(forms.praeteritum)\n",
    "    if forms.perfekt:\n",
    "        forms.perfekt = select_primary(forms.perfekt)\n",
    "\n",
    "    if not any([forms.praeteritum, forms.partizip_ii, forms.perfekt, forms.aux]):\n",
    "        return None\n",
    "    if forms.aux not in (None, \"haben\", \"sein\"):\n",
    "        forms.aux = None\n",
    "    return forms\n",
    "\n",
    "\n",
    "def build_wiktionary_request(lemma: str) -> Tuple[str, Dict[str, str]]:\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"formatversion\": \"2\",\n",
    "        \"titles\": lemma,\n",
    "        \"prop\": \"extracts|langlinks|revisions\",\n",
    "        \"explaintext\": \"1\",\n",
    "        \"origin\": \"*\",\n",
    "        \"lllang\": \"en\",\n",
    "        \"rvslots\": \"main\",\n",
    "        \"rvprop\": \"content\",\n",
    "    }\n",
    "    return WIKTIONARY_API, params\n",
    "\n",
    "\n",
    "def lookup_wiktionary_summary(lemma: str) -> ProviderResponse:\n",
    "    url, params = build_wiktionary_request(lemma)\n",
    "    record, prepared = prepare_request(\"GET\", url, params=params)\n",
    "    response = send_request(prepared)\n",
    "    data = response.json()\n",
    "    page_list = (data.get(\"query\") or {}).get(\"pages\")\n",
    "    page = page_list[0] if isinstance(page_list, list) and page_list else None\n",
    "    if not page or page.get(\"missing\"):\n",
    "        parsed = None\n",
    "    else:\n",
    "        extract = page.get(\"extract\") or \"\"\n",
    "        lines = [line.strip() for line in extract.split(\"\n",
    "\") if line.strip()]\n",
    "        summary = \"\n",
    "\".join(lines[:4]) or None\n",
    "        english_links: List[str] = []\n",
    "        for link in to_array(page.get(\"langlinks\")):\n",
    "            if isinstance(link, dict) and link.get(\"lang\") == \"en\" and link.get(\"title\"):\n",
    "                english_links.append(link[\"title\"].strip())\n",
    "        revisions = to_array(page.get(\"revisions\"))\n",
    "        raw_content = \"\"\n",
    "        if revisions:\n",
    "            raw_content = (revisions[0].get(\"slots\", {}) if isinstance(revisions[0], dict) else {}).get(\"main\", {}).get(\"content\", \"\")\n",
    "        forms = extract_verb_forms(raw_content) if raw_content else None\n",
    "        parsed = WiktionaryLookup(summary=summary, english_hints=english_links, forms=forms)\n",
    "    return ProviderResponse(request=record, raw=data, parsed=parsed)\n",
    "\n",
    "\n",
    "def build_open_thesaurus_request(lemma: str) -> Tuple[str, Dict[str, str]]:\n",
    "    return OPEN_THESAURUS_API, {\"q\": lemma, \"format\": \"application/json\"}\n",
    "\n",
    "\n",
    "def lookup_open_thesaurus_synonyms(lemma: str) -> ProviderResponse:\n",
    "    url, params = build_open_thesaurus_request(lemma)\n",
    "    record, prepared = prepare_request(\"GET\", url, params=params)\n",
    "    response = send_request(prepared)\n",
    "    data = response.json()\n",
    "    collected: List[str] = []\n",
    "    seen = set()\n",
    "    for synset in to_array(data.get(\"synsets\")):\n",
    "        if not isinstance(synset, dict):\n",
    "            continue\n",
    "        for term in to_array(synset.get(\"terms\")):\n",
    "            if not isinstance(term, dict):\n",
    "                continue\n",
    "            value = (term.get(\"term\") or \"\").strip()\n",
    "            if value and value.lower() != lemma.lower() and value not in seen:\n",
    "                collected.append(value)\n",
    "                seen.add(value)\n",
    "    parsed = SynonymLookup(synonyms=collected[:10])\n",
    "    return ProviderResponse(request=record, raw=data, parsed=parsed)\n",
    "\n",
    "\n",
    "def build_translation_request(lemma: str) -> Tuple[str, Dict[str, str]]:\n",
    "    return MY_MEMORY_API, {\"q\": lemma, \"langpair\": \"de|en\"}\n",
    "\n",
    "\n",
    "def lookup_translation(lemma: str) -> ProviderResponse:\n",
    "    url, params = build_translation_request(lemma)\n",
    "    record, prepared = prepare_request(\"GET\", url, params=params)\n",
    "    response = send_request(prepared)\n",
    "    data = response.json()\n",
    "    parsed: Optional[TranslationLookup] = None\n",
    "    for match in to_array(data.get(\"matches\")):\n",
    "        if not isinstance(match, dict):\n",
    "            continue\n",
    "        translation = (match.get(\"translation\") or \"\").strip()\n",
    "        quality = match.get(\"quality\")\n",
    "        if translation and isinstance(quality, (int, float)) and quality >= 80:\n",
    "            parsed = TranslationLookup(translation=translation, source=\"mymemory.translated.net\", confidence=float(quality))\n",
    "            break\n",
    "    if not parsed:\n",
    "        response_data = data.get(\"responseData\") or {}\n",
    "        fallback = response_data.get(\"translatedText\")\n",
    "        if isinstance(fallback, str) and fallback.strip() and fallback.strip().lower() != lemma.lower():\n",
    "            match_value = response_data.get(\"match\")\n",
    "            confidence = float(match_value) * 100 if isinstance(match_value, (int, float)) else None\n",
    "            parsed = TranslationLookup(translation=fallback.strip(), source=\"mymemory.translated.net\", confidence=confidence)\n",
    "    return ProviderResponse(request=record, raw=data, parsed=parsed)\n",
    "\n",
    "\n",
    "def build_tatoeba_request(lemma: str) -> Tuple[str, Dict[str, str]]:\n",
    "    return TATOEBA_API, {\"query\": lemma, \"from\": \"deu\", \"to\": \"eng\", \"sort\": \"relevance\", \"limit\": \"1\"}\n",
    "\n",
    "\n",
    "def lookup_example_sentence(lemma: str) -> ProviderResponse:\n",
    "    url, params = build_tatoeba_request(lemma)\n",
    "    record, prepared = prepare_request(\"GET\", url, params=params)\n",
    "    response = send_request(prepared)\n",
    "    data = response.json()\n",
    "    results = data.get(\"results\") if isinstance(data, dict) else None\n",
    "    result = results[0] if isinstance(results, list) and results else None\n",
    "    parsed: Optional[ExampleLookup] = None\n",
    "    if isinstance(result, dict):\n",
    "        german = (result.get(\"text\") or \"\").strip()\n",
    "        if german:\n",
    "            for translation in to_array(result.get(\"translations\")):\n",
    "                if not isinstance(translation, dict):\n",
    "                    continue\n",
    "                if translation.get(\"lang\") == \"eng\":\n",
    "                    english_text = (translation.get(\"text\") or \"\").strip()\n",
    "                    if english_text:\n",
    "                        parsed = ExampleLookup(example_de=german, example_en=english_text, source=\"tatoeba.org\")\n",
    "                        break\n",
    "    return ProviderResponse(request=record, raw=data, parsed=parsed)\n",
    "\n",
    "\n",
    "def encode_wiktextract_prefix(prefix: str) -> str:\n",
    "    return (\n",
    "        prefix\n",
    "        .replace(\"/\", \"_slash_\")\n",
    "        .replace(\"\\\", \"_backslash_\")\n",
    "        .replace(\"*\", \"_star_\")\n",
    "        .replace(\"?\", \"_ques_\")\n",
    "        .replace(\"#\", \"_hash_\")\n",
    "        .replace(\".\", \"_dot_\")\n",
    "    )\n",
    "\n",
    "\n",
    "def resolve_wiktextract_url(lemma: str) -> Tuple[Optional[str], List[RequestRecord]]:\n",
    "    trimmed = lemma.strip()\n",
    "    if not trimmed:\n",
    "        return None, []\n",
    "    lower = trimmed.lower()\n",
    "    history: List[RequestRecord] = []\n",
    "    for length in dict.fromkeys([3, 2, 1]):\n",
    "        if length > len(lower):\n",
    "            continue\n",
    "        prefix = lower[:length]\n",
    "        search_url = f\"{WIKTEXTRACT_SEARCH_API}/{encode_wiktextract_prefix(prefix)}.json\"\n",
    "        record, prepared = prepare_request(\"GET\", search_url)\n",
    "        history.append(record)\n",
    "        response = send_request(prepared)\n",
    "        data = response.json()\n",
    "        entries = data[1] if isinstance(data, list) and len(data) > 1 else []\n",
    "        direct = next((item for item in entries if item[0].lower() == lower), None)\n",
    "        if direct:\n",
    "            return direct[1], history\n",
    "        loose = next((item for item in entries if item[0].lower().startswith(lower)), None)\n",
    "        if loose:\n",
    "            return loose[1], history\n",
    "    return None, history\n",
    "\n",
    "\n",
    "def lookup_wiktextract(lemma: str) -> ProviderResponse:\n",
    "    url, history = resolve_wiktextract_url(lemma)\n",
    "    if not url:\n",
    "        return ProviderResponse(\n",
    "            request=RequestRecord(method=\"GET\", url=\"\", params={}, headers={}),\n",
    "            raw=None,\n",
    "            parsed=None,\n",
    "            history=history,\n",
    "        )\n",
    "    record, prepared = prepare_request(\"GET\", url)\n",
    "    response = send_request(prepared)\n",
    "    html = response.text\n",
    "    marker_index = html.find(WIKTEXTRACT_RAW_MARKER)\n",
    "    raw_data = None\n",
    "    if marker_index != -1:\n",
    "        pre_start = html.find(\"<pre>\", marker_index)\n",
    "        pre_end = html.find(\"</pre>\", pre_start)\n",
    "        if pre_start != -1 and pre_end != -1:\n",
    "            raw_json = html[pre_start + 5:pre_end]\n",
    "            raw_json = unescape(raw_json)\n",
    "            raw_data = json.loads(raw_json)\n",
    "    if not isinstance(raw_data, dict):\n",
    "        return ProviderResponse(request=record, raw=html, parsed=None, history=history)\n",
    "    translations: List[str] = []\n",
    "    for translation in to_array(raw_data.get(\"translations\")):\n",
    "        if not isinstance(translation, dict):\n",
    "            continue\n",
    "        lang = (translation.get(\"lang\") or \"\").lower()\n",
    "        code = (translation.get(\"lang_code\") or \"\").lower()\n",
    "        word = (translation.get(\"word\") or \"\").strip()\n",
    "        if not word or word.lower() == lemma.lower():\n",
    "            continue\n",
    "        if lang.startswith(\"engl\") or code == \"en\":\n",
    "            translations.append(word)\n",
    "    synonyms: List[str] = []\n",
    "    for synonym in to_array(raw_data.get(\"synonyms\")):\n",
    "        if isinstance(synonym, dict) and synonym.get(\"word\"):\n",
    "            synonyms.append(synonym[\"word\"].strip())\n",
    "    example: Optional[ExampleLookup] = None\n",
    "    for sense in to_array(raw_data.get(\"senses\")):\n",
    "        if not isinstance(sense, dict):\n",
    "            continue\n",
    "        for candidate in to_array(sense.get(\"examples\")):\n",
    "            if not isinstance(candidate, dict):\n",
    "                continue\n",
    "            example_de = (candidate.get(\"text\") or \"\").strip()\n",
    "            example_en = (candidate.get(\"translation\") or \"\").strip()\n",
    "            if example_de or example_en:\n",
    "                example = ExampleLookup(example_de=example_de or None, example_en=example_en or None, source=\"kaikki.org\")\n",
    "                if example.example_en:\n",
    "                    break\n",
    "        if example and example.example_en:\n",
    "            break\n",
    "    parsed = None\n",
    "    if translations or synonyms or example:\n",
    "        parsed = WiktextractLookup(translations=translations[:10], synonyms=synonyms, example=example)\n",
    "    return ProviderResponse(request=record, raw=raw_data, parsed=parsed, history=history)\n",
    "\n",
    "\n",
    "def build_openai_request(lemma: str, pos: str) -> Tuple[str, Dict[str, Any]]:\n",
    "    body = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"temperature\": 0.2,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a linguistics assistant that responds with valid JSON only. Provide translations and simple bilingual example sentences for German vocabulary.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": f\"Return a JSON object with keys translation, exampleDe, exampleEn for the German {pos} \"{lemma}\". Use neutral tone and CEFR A2 difficulty. If unsure, omit the key.\",\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "        \"response_format\": {\"type\": \"json_object\"},\n",
    "    }\n",
    "    return OPENAI_CHAT_COMPLETIONS, body\n",
    "\n",
    "\n",
    "def lookup_ai_assistance(lemma: str, pos: str, api_key: Optional[str]) -> ProviderResponse:\n",
    "    if not api_key:\n",
    "        return ProviderResponse(\n",
    "            request=RequestRecord(method=\"POST\", url=OPENAI_CHAT_COMPLETIONS, params={}, headers={}),\n",
    "            raw=None,\n",
    "            parsed=None,\n",
    "        )\n",
    "    _, body = build_openai_request(lemma, pos)\n",
    "    headers = {**REQUEST_HEADERS, \"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "    record, prepared = prepare_request(\"POST\", OPENAI_CHAT_COMPLETIONS, headers=headers, json_body=body)\n",
    "    response = send_request(prepared)\n",
    "    data = response.json()\n",
    "    content = None\n",
    "    choices = data.get(\"choices\")\n",
    "    if isinstance(choices, list) and choices:\n",
    "        content = choices[0].get(\"message\", {}).get(\"content\")\n",
    "    parsed = None\n",
    "    if isinstance(content, str):\n",
    "        try:\n",
    "            payload = json.loads(content)\n",
    "            parsed = {\n",
    "                \"translation\": payload.get(\"translation\"),\n",
    "                \"example\": ExampleLookup(\n",
    "                    example_de=payload.get(\"exampleDe\"),\n",
    "                    example_en=payload.get(\"exampleEn\"),\n",
    "                    source=\"openai.com\",\n",
    "                ),\n",
    "            }\n",
    "        except json.JSONDecodeError:\n",
    "            parsed = None\n",
    "    return ProviderResponse(request=record, raw=data, parsed=parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c42c428",
   "metadata": {},
   "source": [
    "## Wiktionary — Summary, English hints, and verb forms\n",
    "\n",
    "**API URL:** `https://de.wiktionary.org/w/api.php`\n",
    "\n",
    "**Sample request parameters:**\n",
    "```python\n",
    "url, params = build_wiktionary_request(\"machen\")\n",
    "print(url)\n",
    "print(params)\n",
    "```\n",
    "\n",
    "Run the next cell to send the request, inspect the raw JSON, and view the parsed payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84707933",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = \"machen\"\n",
    "wiktionary = lookup_wiktionary_summary(lemma)\n",
    "print(wiktionary.request)\n",
    "print(json.dumps(wiktionary.raw, indent=2)[:1000])\n",
    "print(wiktionary.parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d0a4a1",
   "metadata": {},
   "source": [
    "## OpenThesaurus — Synonym lookup\n",
    "\n",
    "**API URL:** `https://www.openthesaurus.de/synonyme/search`\n",
    "\n",
    "**Sample request parameters:**\n",
    "```python\n",
    "url, params = build_open_thesaurus_request(\"machen\")\n",
    "print(url)\n",
    "print(params)\n",
    "```\n",
    "\n",
    "The next cell fetches synonyms and shows how the enrichment pipeline filters duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a888cf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = lookup_open_thesaurus_synonyms(lemma)\n",
    "print(synonyms.request)\n",
    "print(json.dumps(synonyms.raw, indent=2)[:1000])\n",
    "print(synonyms.parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4931975",
   "metadata": {},
   "source": [
    "## MyMemory — Translation lookup\n",
    "\n",
    "**API URL:** `https://api.mymemory.translated.net/get`\n",
    "\n",
    "**Sample request parameters:**\n",
    "```python\n",
    "url, params = build_translation_request(\"machen\")\n",
    "print(url)\n",
    "print(params)\n",
    "```\n",
    "\n",
    "Use the next cell to retrieve translations and inspect both the scored matches and fallback response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b8482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = lookup_translation(lemma)\n",
    "print(translation.request)\n",
    "print(json.dumps(translation.raw, indent=2)[:1000])\n",
    "print(translation.parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da67ee5",
   "metadata": {},
   "source": [
    "## Tatoeba — Example sentence lookup\n",
    "\n",
    "**API URL:** `https://tatoeba.org/en/api_v0/search`\n",
    "\n",
    "**Sample request parameters:**\n",
    "```python\n",
    "url, params = build_tatoeba_request(\"machen\")\n",
    "print(url)\n",
    "print(params)\n",
    "```\n",
    "\n",
    "Call the helper below to see the bilingual example the enrichment pipeline would store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a9bd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = lookup_example_sentence(lemma)\n",
    "print(example.request)\n",
    "print(json.dumps(example.raw, indent=2)[:1000])\n",
    "print(example.parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652fb47c",
   "metadata": {},
   "source": [
    "## Wiktextract — Additional translations, synonyms, and examples\n",
    "\n",
    "**Search API URL:** `https://kaikki.org/dewiktionary/search/start/<prefix>.json`\n",
    "\n",
    "**Sample request:** the code resolves the correct JSON detail URL automatically.\n",
    "\n",
    "Execute the following cell to inspect the Kaikki HTML payload and the parsed enrichment result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073f0ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiktextract = lookup_wiktextract(lemma)\n",
    "print(wiktextract.history)\n",
    "print(wiktextract.request)\n",
    "print(str(wiktextract.raw)[:1000])\n",
    "print(wiktextract.parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f51eb",
   "metadata": {},
   "source": [
    "## Optional: OpenAI helper\n",
    "\n",
    "The enrichment pipeline can fall back to OpenAI for suggestions when configured. This is optional and requires an API key.\n",
    "\n",
    "```python\n",
    "import os\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "ai_result = lookup_ai_assistance(lemma, \"verb\", api_key)\n",
    "print(ai_result.request)\n",
    "print(json.dumps(ai_result.raw, indent=2)[:1000])\n",
    "print(ai_result.parsed)\n",
    "```\n",
    "\n",
    "The call is skipped automatically when `OPENAI_API_KEY` is unset."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
